{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec4e4ebe-4313-499b-b5c7-afc3ec028660",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Violence Detection in Surveillance from Drones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f459b4a7-71c8-445a-ad74-8f37fe6ab987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3191739-c160-4a3f-bcc0-6f3549ae2a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'pyskl' already exists and is not an empty directory.\n",
      "Obtaining file:///home/jovyan/pipe/pyskl\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: decord>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from pyskl==0.1.0) (0.6.0)\n",
      "Requirement already satisfied: fvcore in /opt/conda/lib/python3.9/site-packages (from pyskl==0.1.0) (0.1.5.post20221221)\n",
      "Requirement already satisfied: matplotlib in /home/jovyan/.local/lib/python3.9/site-packages (from pyskl==0.1.0) (3.7.1)\n",
      "Requirement already satisfied: mmcv-full==1.5.0 in /opt/conda/lib/python3.9/site-packages (from pyskl==0.1.0) (1.5.0)\n",
      "Requirement already satisfied: mmdet==2.23.0 in /opt/conda/lib/python3.9/site-packages (from pyskl==0.1.0) (2.23.0)\n",
      "Requirement already satisfied: mmpose==0.24.0 in /opt/conda/lib/python3.9/site-packages (from pyskl==0.1.0) (0.24.0)\n",
      "Requirement already satisfied: moviepy in /home/jovyan/.local/lib/python3.9/site-packages (from pyskl==0.1.0) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.9/site-packages (from pyskl==0.1.0) (1.23.5)\n",
      "Requirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.9/site-packages (from pyskl==0.1.0) (4.7.0.72)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.9/site-packages (from pyskl==0.1.0) (4.7.0.72)\n",
      "Requirement already satisfied: pymemcache in /opt/conda/lib/python3.9/site-packages (from pyskl==0.1.0) (4.0.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from pyskl==0.1.0) (1.10.1)\n",
      "Requirement already satisfied: torch>=1.5 in /opt/conda/lib/python3.9/site-packages (from pyskl==0.1.0) (2.0.1+cu117)\n",
      "Requirement already satisfied: tqdm in /home/jovyan/.local/lib/python3.9/site-packages (from pyskl==0.1.0) (4.65.0)\n",
      "Requirement already satisfied: addict in /opt/conda/lib/python3.9/site-packages (from mmcv-full==1.5.0->pyskl==0.1.0) (2.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from mmcv-full==1.5.0->pyskl==0.1.0) (23.1)\n",
      "Requirement already satisfied: Pillow in /home/jovyan/.local/lib/python3.9/site-packages (from mmcv-full==1.5.0->pyskl==0.1.0) (9.5.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from mmcv-full==1.5.0->pyskl==0.1.0) (6.0)\n",
      "Requirement already satisfied: yapf in /opt/conda/lib/python3.9/site-packages (from mmcv-full==1.5.0->pyskl==0.1.0) (0.33.0)\n",
      "Requirement already satisfied: pycocotools in /opt/conda/lib/python3.9/site-packages (from mmdet==2.23.0->pyskl==0.1.0) (2.0.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from mmdet==2.23.0->pyskl==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: terminaltables in /opt/conda/lib/python3.9/site-packages (from mmdet==2.23.0->pyskl==0.1.0) (3.1.10)\n",
      "Requirement already satisfied: chumpy in /opt/conda/lib/python3.9/site-packages (from mmpose==0.24.0->pyskl==0.1.0) (0.70)\n",
      "Requirement already satisfied: json-tricks in /opt/conda/lib/python3.9/site-packages (from mmpose==0.24.0->pyskl==0.1.0) (3.16.1)\n",
      "Requirement already satisfied: munkres in /opt/conda/lib/python3.9/site-packages (from mmpose==0.24.0->pyskl==0.1.0) (1.1.4)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.9/site-packages (from mmpose==0.24.0->pyskl==0.1.0) (0.15.2+cu117)\n",
      "Requirement already satisfied: xtcocotools>=1.8 in /home/jovyan/.local/lib/python3.9/site-packages (from mmpose==0.24.0->pyskl==0.1.0) (1.13)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from torch>=1.5->pyskl==0.1.0) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.5->pyskl==0.1.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.9/site-packages (from torch>=1.5->pyskl==0.1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.9/site-packages (from torch>=1.5->pyskl==0.1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from torch>=1.5->pyskl==0.1.0) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.9/site-packages (from torch>=1.5->pyskl==0.1.0) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.5->pyskl==0.1.0) (3.26.3)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.5->pyskl==0.1.0) (16.0.3)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /opt/conda/lib/python3.9/site-packages (from fvcore->pyskl==0.1.0) (0.1.8)\n",
      "Requirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.9/site-packages (from fvcore->pyskl==0.1.0) (2.3.0)\n",
      "Requirement already satisfied: tabulate in /home/jovyan/.local/lib/python3.9/site-packages (from fvcore->pyskl==0.1.0) (0.9.0)\n",
      "Requirement already satisfied: iopath>=0.1.7 in /opt/conda/lib/python3.9/site-packages (from fvcore->pyskl==0.1.0) (0.1.10)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/jovyan/.local/lib/python3.9/site-packages (from matplotlib->pyskl==0.1.0) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jovyan/.local/lib/python3.9/site-packages (from matplotlib->pyskl==0.1.0) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jovyan/.local/lib/python3.9/site-packages (from matplotlib->pyskl==0.1.0) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/jovyan/.local/lib/python3.9/site-packages (from matplotlib->pyskl==0.1.0) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/jovyan/.local/lib/python3.9/site-packages (from matplotlib->pyskl==0.1.0) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib->pyskl==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/jovyan/.local/lib/python3.9/site-packages (from matplotlib->pyskl==0.1.0) (5.12.0)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /home/jovyan/.local/lib/python3.9/site-packages (from moviepy->pyskl==0.1.0) (4.4.2)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from moviepy->pyskl==0.1.0) (2.29.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /opt/conda/lib/python3.9/site-packages (from moviepy->pyskl==0.1.0) (0.1.10)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /opt/conda/lib/python3.9/site-packages (from moviepy->pyskl==0.1.0) (2.28.1)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /opt/conda/lib/python3.9/site-packages (from moviepy->pyskl==0.1.0) (0.4.8)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->pyskl==0.1.0) (3.15.0)\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.9/site-packages (from iopath>=0.1.7->fvcore->pyskl==0.1.0) (2.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy->pyskl==0.1.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy->pyskl==0.1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy->pyskl==0.1.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy->pyskl==0.1.0) (2023.5.7)\n",
      "Requirement already satisfied: setuptools>=18.0 in /opt/conda/lib/python3.9/site-packages (from xtcocotools>=1.8->mmpose==0.24.0->pyskl==0.1.0) (67.7.2)\n",
      "Requirement already satisfied: cython>=0.27.3 in /home/jovyan/.local/lib/python3.9/site-packages (from xtcocotools>=1.8->mmpose==0.24.0->pyskl==0.1.0) (0.29.34)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->torch>=1.5->pyskl==0.1.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.9/site-packages (from sympy->torch>=1.5->pyskl==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /opt/conda/lib/python3.9/site-packages (from yapf->mmcv-full==1.5.0->pyskl==0.1.0) (2.0.1)\n",
      "Installing collected packages: pyskl\n",
      "  Attempting uninstall: pyskl\n",
      "    Found existing installation: pyskl 0.1.0\n",
      "    Uninstalling pyskl-0.1.0:\n",
      "      Successfully uninstalled pyskl-0.1.0\n",
      "  Running setup.py develop for pyskl\n",
      "Successfully installed pyskl-0.1.0\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/kennymckormick/pyskl.git\n",
    "os.chdir(\"./pyskl\")\n",
    "# ! conda env create -f pyskl.yaml\n",
    "# ! conda activate pyskl\n",
    "! pip install -e .\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f38339d0-5d57-4fab-8de3-df8eb2250a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import torch\n",
    "import warnings\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import tqdm\n",
    "from pyskl.apis import inference_recognizer, init_recognizer\n",
    "\n",
    "try:\n",
    "    from mmdet.apis import inference_detector, init_detector\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    def inference_detector(*args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def init_detector(*args, **kwargs):\n",
    "        pass\n",
    "    warnings.warn(\n",
    "        'Failed to import `inference_detector` and `init_detector` from `mmdet.apis`. '\n",
    "        'Make sure you can successfully import these if you want to use related features. '\n",
    "    )\n",
    "\n",
    "try:\n",
    "    from mmpose.apis import inference_top_down_pose_model, init_pose_model, vis_pose_result\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    def init_pose_model(*args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def inference_top_down_pose_model(*args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def vis_pose_result(*args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    warnings.warn(\n",
    "        'Failed to import `init_pose_model`, `inference_top_down_pose_model`, `vis_pose_result` from '\n",
    "        '`mmpose.apis`. Make sure you can successfully import these if you want to use related features. '\n",
    "    )\n",
    "\n",
    "\n",
    "try:\n",
    "    import moviepy.editor as mpy\n",
    "except ImportError:\n",
    "    raise ImportError('Please install moviepy to enable output file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7fb89d5-8734-4968-a9b9-e2cb3bc80da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTFACE = cv2.FONT_HERSHEY_DUPLEX\n",
    "FONTSCALE = 1.5\n",
    "FONTCOLOR = (0, 0, 0)  # BGR, white\n",
    "THICKNESS = 2\n",
    "LINETYPE = 1\n",
    "BATCH_SIZE = 20\n",
    "def parse_args():\n",
    "    config = {}\n",
    "\n",
    "    args = argparse.Namespace(\n",
    "        video='test.avi', \n",
    "        out_filename='test2.mp4', \n",
    "        config='configs/posec3d/slowonly_r50_ntu120_xsub/joint.py', \n",
    "        checkpoint='https://download.openmmlab.com/mmaction/pyskl/ckpt/posec3d/slowonly_r50_ntu120_xsub/joint.pth', \n",
    "        det_config='faster_rcnn_r50_fpn_1x_coco-person.py', \n",
    "        det_checkpoint='https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco-person/faster_rcnn_r50_fpn_1x_coco-person_20201216_175929-d022e227.pth', \n",
    "        pose_config='hrnet_w32_coco_256x192.py', \n",
    "        pose_checkpoint='https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth', \n",
    "        det_score_thr=0.9, \n",
    "        label_map='tools/data/label_map/nturgbd_120.txt', \n",
    "        device='cuda:0', \n",
    "        short_side=480)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8c0e5eb-afe9-454b-9b35-cbedf8acc75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_inference(args, frame_paths, det_results):\n",
    "    model = init_pose_model(POSE_CONFIG, POSE_CHECKPOINT,\n",
    "                            DEVICE)\n",
    "    ret = []\n",
    "    print('Performing Human Pose Estimation for each frame')\n",
    "    prog_bar = mmcv.ProgressBar(len(frame_paths))\n",
    "    for f, d in zip(frame_paths, det_results):\n",
    "        # Align input format\n",
    "        d = [dict(bbox=x) for x in list(d)]\n",
    "        pose = inference_top_down_pose_model(model, f, d, format='xyxy')[0]\n",
    "        ret.append(pose)\n",
    "        prog_bar.update()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87644fdc-1c89-4e7b-8e45-28bf0f85d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_ske(ske1, ske2):\n",
    "    dist = np.linalg.norm(ske1[:, :2] - ske2[:, :2], axis=1) * 2\n",
    "    diff = np.abs(ske1[:, 2] - ske2[:, 2])\n",
    "    return np.sum(np.maximum(dist, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80f39d60-b11f-4aa6-b9c3-e706660a6039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_extraction(video_path, short_side):\n",
    "    \"\"\"Extract frames given video_path.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): The video_path.\n",
    "    \"\"\"\n",
    "    # Load the video, extract frames into ./tmp/video_name\n",
    "    target_dir = osp.join('./tmp', osp.basename(osp.splitext(video_path)[0]))\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    # Should be able to handle videos up to several hours\n",
    "    frame_tmpl = osp.join(target_dir, 'img_{:06d}.jpg')\n",
    "    vid = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_paths = []\n",
    "    flag, frame = vid.read()\n",
    "    cnt = 0\n",
    "    new_h, new_w = None, None\n",
    "    while flag:\n",
    "        if new_h is None:\n",
    "            h, w, _ = frame.shape\n",
    "            new_w, new_h = mmcv.rescale_size((w, h), (short_side, np.Inf))\n",
    "\n",
    "        frame = mmcv.imresize(frame, (new_w, new_h))\n",
    "\n",
    "        frames.append(frame)\n",
    "        frame_path = frame_tmpl.format(cnt + 1)\n",
    "        frame_paths.append(frame_path)\n",
    "\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        cnt += 1\n",
    "        flag, frame = vid.read()\n",
    "\n",
    "    return frame_paths, frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "387e6da6-64cf-4ddc-8ba5-610069df6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_inference(args, frame_paths):\n",
    "    \"\"\"Detect human boxes given frame paths.\n",
    "\n",
    "    Args:\n",
    "        args (argparse.Namespace): The arguments.\n",
    "        frame_paths (list[str]): The paths of frames to do detection inference.\n",
    "\n",
    "    Returns:\n",
    "        list[np.ndarray]: The human detection results.\n",
    "    \"\"\"\n",
    "    model = init_detector(args.det_config, args.det_checkpoint, args.device)\n",
    "    assert model is not None, ('Failed to build the detection model. Check if you have installed mmcv-full properly. '\n",
    "                               'You should first install mmcv-full successfully, then install mmdet, mmpose. ')\n",
    "    assert model.CLASSES[0] == 'person', 'We require you to use a detector trained on COCO'\n",
    "    results = []\n",
    "    print('Performing Human Detection for each frame')\n",
    "    prog_bar = mmcv.ProgressBar(len(frame_paths))\n",
    "    for frame_path in frame_paths:\n",
    "        result = inference_detector(model, frame_path)\n",
    "        # We only keep human detections with score larger than det_score_thr\n",
    "        result = result[0][result[0][:, 4] >= args.det_score_thr]\n",
    "        results.append(result)\n",
    "        prog_bar.update()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84ae3f40-e8ec-4dac-b9e2-edd6cab5b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_inference(args, frame_paths, det_results):\n",
    "    model = init_pose_model(args.pose_config, args.pose_checkpoint,\n",
    "                            args.device)\n",
    "    ret = []\n",
    "    print('Performing Human Pose Estimation for each frame')\n",
    "    prog_bar = mmcv.ProgressBar(len(frame_paths))\n",
    "    for f, d in zip(frame_paths, det_results):\n",
    "        # Align input format\n",
    "        d = [dict(bbox=x) for x in list(d)]\n",
    "        pose = inference_top_down_pose_model(model, f, d, format='xyxy')[0]\n",
    "        ret.append(pose)\n",
    "        prog_bar.update()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31b4be9c-a1bc-484a-8f20-f96d65248e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_ske(ske1, ske2):\n",
    "    dist = np.linalg.norm(ske1[:, :2] - ske2[:, :2], axis=1) * 2\n",
    "    diff = np.abs(ske1[:, 2] - ske2[:, 2])\n",
    "    return np.sum(np.maximum(dist, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb3fea8f-cfef-4b38-8e81-d28eb37f30bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_tracking(pose_results, max_tracks=2, thre=30):\n",
    "    tracks, num_tracks = [], 0\n",
    "    num_joints = None\n",
    "    for idx, poses in enumerate(pose_results):\n",
    "        if len(poses) == 0:\n",
    "            continue\n",
    "        if num_joints is None:\n",
    "            num_joints = poses[0].shape[0]\n",
    "        track_proposals = [t for t in tracks if t['data'][-1][0] > idx - thre]\n",
    "        n, m = len(track_proposals), len(poses)\n",
    "        scores = np.zeros((n, m))\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                scores[i][j] = dist_ske(track_proposals[i]['data'][-1][1], poses[j])\n",
    "\n",
    "        row, col = linear_sum_assignment(scores)\n",
    "        for r, c in zip(row, col):\n",
    "            track_proposals[r]['data'].append((idx, poses[c]))\n",
    "        if m > n:\n",
    "            for j in range(m):\n",
    "                if j not in col:\n",
    "                    num_tracks += 1\n",
    "                    new_track = dict(data=[])\n",
    "                    new_track['track_id'] = num_tracks\n",
    "                    new_track['data'] = [(idx, poses[j])]\n",
    "                    tracks.append(new_track)\n",
    "    tracks.sort(key=lambda x: -len(x['data']))\n",
    "    # print(pose_results, num_joints)\n",
    "    if num_joints == None:\n",
    "        return None\n",
    "    result = np.zeros((max_tracks, len(pose_results), num_joints, 3), dtype=np.float16)\n",
    "    for i, track in enumerate(tracks[:max_tracks]):\n",
    "        for item in track['data']:\n",
    "            idx, pose = item\n",
    "            result[i, idx] = pose\n",
    "    return result[..., :2], result[..., 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f631dde-6a37-4602-a02c-79e1564d3e53",
   "metadata": {},
   "source": [
    "## Обработка видеозаписи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "910c3773-c6bb-4392-bc91-0c1949ab102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()\n",
    "\n",
    "frame_paths_l, original_frames_l = frame_extraction(args.video,\n",
    "                                                    args.short_side)    \n",
    "result_frames = []\n",
    "\n",
    "videos_cnt = len(frame_paths_l) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "257bf7cf-26c9-497f-b967-1a7a2eb8e159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: .cache/joint_f6bed715.pth\n",
      "load checkpoint from http path: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco-person/faster_rcnn_r50_fpn_1x_coco-person_20201216_175929-d022e227.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 7.3 task/s, elapsed: 3s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 4.1 task/s, elapsed: 5s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:19<00:38, 19.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: .cache/joint_f6bed715.pth\n",
      "load checkpoint from http path: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco-person/faster_rcnn_r50_fpn_1x_coco-person_20201216_175929-d022e227.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.5 task/s, elapsed: 2s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 5.2 task/s, elapsed: 4s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:35<00:17, 17.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: .cache/joint_f6bed715.pth\n",
      "load checkpoint from http path: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco-person/faster_rcnn_r50_fpn_1x_coco-person_20201216_175929-d022e227.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 10.9 task/s, elapsed: 2s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 20/20, 4.0 task/s, elapsed: 5s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:50<00:00, 16.92s/it]\n"
     ]
    }
   ],
   "source": [
    "for ef in tqdm.tqdm(range(videos_cnt)):\n",
    "    frame_paths = frame_paths_l[ef*BATCH_SIZE:(ef+1)*BATCH_SIZE]\n",
    "    original_frames = original_frames_l[ef*BATCH_SIZE:(ef+1)*BATCH_SIZE]\n",
    "\n",
    "    num_frame = len(frame_paths)\n",
    "    h, w, _ = original_frames[0].shape\n",
    "\n",
    "    config = mmcv.Config.fromfile(args.config)\n",
    "    config.data.test.pipeline = [x for x in config.data.test.pipeline if x['type'] != 'DecompressPose']\n",
    "    # Are we using GCN for Infernece?\n",
    "    GCN_flag = 'GCN' in config.model.type\n",
    "    GCN_nperson = None\n",
    "    if GCN_flag:\n",
    "        format_op = [op for op in config.data.test.pipeline if op['type'] == 'FormatGCNInput'][0]\n",
    "        # We will set the default value of GCN_nperson to 2, which is\n",
    "        # the default arg of FormatGCNInput\n",
    "        GCN_nperson = format_op.get('num_person', 2)\n",
    "\n",
    "    model = init_recognizer(config, args.checkpoint, args.device)\n",
    "\n",
    "\n",
    "    # Load label_map\n",
    "    label_map = [x.strip() for x in open(args.label_map).readlines()]\n",
    "\n",
    "    # Get Human detection results\n",
    "    det_results = detection_inference(args, frame_paths)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    pose_results = pose_inference(args, frame_paths, det_results)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    fake_anno = dict(\n",
    "        frame_dir='',\n",
    "        label=-1,\n",
    "        img_shape=(h, w),\n",
    "        original_shape=(h, w),\n",
    "        start_index=0,\n",
    "        modality='Pose',\n",
    "        total_frames=num_frame)\n",
    "\n",
    "\n",
    "    if len(pose_results[-1]) > 0:\n",
    "        if GCN_flag:\n",
    "            # We will keep at most `GCN_nperson` persons per frame.\n",
    "            tracking_inputs = [[pose['keypoints'] for pose in poses] for poses in pose_results]\n",
    "            keypoint, keypoint_score = pose_tracking(tracking_inputs, max_tracks=GCN_nperson)\n",
    "            fake_anno['keypoint'] = keypoint\n",
    "            fake_anno['keypoint_score'] = keypoint_score\n",
    "        else:\n",
    "            num_person = max([len(x) for x in pose_results])\n",
    "            # Current PoseC3D models are trained on COCO-keypoints (17 keypoints)\n",
    "            num_keypoint = 17\n",
    "            keypoint = np.zeros((num_person, num_frame, num_keypoint, 2),\n",
    "                                dtype=np.float16)\n",
    "            keypoint_score = np.zeros((num_person, num_frame, num_keypoint),\n",
    "                                      dtype=np.float16)\n",
    "            for i, poses in enumerate(pose_results):\n",
    "                for j, pose in enumerate(poses):\n",
    "                    pose = pose['keypoints']\n",
    "                    keypoint[j, i] = pose[:, :2]\n",
    "                    keypoint_score[j, i] = pose[:, 2]\n",
    "            fake_anno['keypoint'] = keypoint\n",
    "            fake_anno['keypoint_score'] = keypoint_score\n",
    "\n",
    "    else:\n",
    "        num_keypoint = 17\n",
    "        keypoint = np.zeros((0, num_frame, num_keypoint, 2),\n",
    "                                dtype=np.float16)\n",
    "        keypoint_score = np.zeros((0, num_frame, num_keypoint),\n",
    "                                  dtype=np.float16)\n",
    "        fake_anno['keypoint'] = keypoint\n",
    "        fake_anno['keypoint_score'] = keypoint_score\n",
    "\n",
    "    results = inference_recognizer(model, fake_anno)\n",
    "\n",
    "\n",
    "    action_label = label_map[results[0][0]]\n",
    "\n",
    "    pose_model = init_pose_model(args.pose_config, args.pose_checkpoint,\n",
    "                                 args.device)\n",
    "    vis_frames = [\n",
    "        vis_pose_result(pose_model, frame_paths[i], pose_results[i])\n",
    "        for i in range(num_frame)\n",
    "    ]\n",
    "    for frame in vis_frames:\n",
    "        if action_label == \"fight\":\n",
    "            REDCOLOR = (0, 0, 255)  # BGR, white\n",
    "            cv2.putText(frame, \"fight\", (10, 30), FONTFACE, FONTSCALE,\n",
    "                        REDCOLOR, THICKNESS, LINETYPE)\n",
    "        else:\n",
    "            cv2.putText(frame, action_label, (10, 30), FONTFACE, FONTSCALE,\n",
    "                        FONTCOLOR, THICKNESS, LINETYPE)\n",
    "\n",
    "\n",
    "    result_frames += vis_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be72420a-f79a-4317-9482-e9341bbe7e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video test2.mp4.\n",
      "Moviepy - Writing video test2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready test2.mp4\n"
     ]
    }
   ],
   "source": [
    "vid = mpy.ImageSequenceClip([x[:, :, ::-1] for x in result_frames], fps=24)\n",
    "vid.write_videofile(args.out_filename, remove_temp=True)\n",
    "\n",
    "tmp_frame_dir = osp.dirname(frame_paths[0])\n",
    "shutil.rmtree(tmp_frame_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612630b6-cee1-48bb-80bc-fe4053db817e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
